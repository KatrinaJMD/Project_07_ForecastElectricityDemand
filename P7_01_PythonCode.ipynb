{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P9_01_code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical forecasting : energy demand\n",
        "---"
      ],
      "metadata": {
        "id": "MFuwD8zFX_mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time series definition"
      ],
      "metadata": {
        "id": "LR37HICCYZJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time series = Signal + Noise\n",
        "\n",
        "**Signal** : forecasts extrapolate signal portion of model\n",
        "  - Trend/Cycle\n",
        "  - Seasonal\n",
        "\n",
        "**Noise** : confidence intervals account for uncertainty\n",
        "  - Error/Residuals/Remainder/Irregularities"
      ],
      "metadata": {
        "id": "Jhht8rZiW46w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data construction"
      ],
      "metadata": {
        "id": "cIH8S96-Zh0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Settings"
      ],
      "metadata": {
        "id": "CsEpKHcTKzbG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZh3tGdaJ3My"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### libraries"
      ],
      "metadata": {
        "id": "fAeDG0x9qqJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np, warnings, itertools\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import scipy.stats as st\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "from math import sqrt\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import *\n",
        "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from scipy.stats import kstest, norm, shapiro\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from pandas.plotting import autocorrelation_plot\n",
        "\n",
        "!pip install pmdarima\n",
        "from pmdarima import auto_arima\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "DIUwYXX5KAMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### params"
      ],
      "metadata": {
        "id": "vFqKT9Xjqtdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mpl.rcParams['text.color'] = 'G'\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "params = {'legend.fontsize':'x-large',\n",
        "          'figure.figsize':(15, 8),\n",
        "          'lines.linewidth':1.5,\n",
        "          'axes.labelsize':'x-large',\n",
        "          'axes.labelpad':15,\n",
        "          'axes.labelweight':'bold',\n",
        "          'axes.titlesize':35,\n",
        "          'axes.titleweight':'bold',\n",
        "          'xtick.labelsize':'x-large',\n",
        "          'ytick.labelsize':'x-large'}\n",
        "mpl.rcParams.update(params)"
      ],
      "metadata": {
        "id": "StKZDcgAKoqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### functions"
      ],
      "metadata": {
        "id": "fvd4fbpQqvYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction affichant les 5 premières meilleurs combinaisaons de paramètres SARIMA\n",
        "# En fonction de l'AIC et du BIC\n",
        "def sarimax_gridsearch(ts, pdq, pdqs):\n",
        "    '''\n",
        "    Input: \n",
        "        ts : your time series data\n",
        "        pdq : ARIMA combinations from above\n",
        "        pdqs : seasonal ARIMA combinations from above\n",
        "        maxiter : number of iterations, increase if your model isn't converging\n",
        "        frequency : default='M' for month. Change to suit your time series frequency\n",
        "            e.g. 'D' for day, 'H' for hour, 'Y' for year. \n",
        "        \n",
        "    Return:\n",
        "        Prints out top 5 parameter combinations\n",
        "        Returns dataframe of parameter combinations ranked by BIC\n",
        "    '''\n",
        "\n",
        "    # Run a grid search with pdq and seasonal pdq parameters and get the best BIC value\n",
        "    ans = []\n",
        "    for comb in pdq:\n",
        "        for combs in pdqs:\n",
        "            try:\n",
        "                mod = SARIMAX(ts,order=comb, seasonal_order=combs)\n",
        "                output = mod.fit() \n",
        "                ans.append([comb, combs, output.bic, output.aic])\n",
        "                #print('SARIMAX {} x {}12 : BIC Calculated ={} ; AIC Calculated : {}'.format(comb, combs, output.bic, output.aic))\n",
        "            except:\n",
        "                continue\n",
        "            \n",
        "    # Find the parameters with minimal BIC value\n",
        "\n",
        "    # Convert into dataframe\n",
        "    ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'bic', 'aic'])\n",
        "\n",
        "    # Sort and return top 5 combinations\n",
        "    ans_df = ans_df.sort_values(by=['bic', 'aic'],ascending=True)\n",
        "    \n",
        "    return ans_df.head()\n",
        "\n",
        "# fonction affichant le Test de Dickey–Fuller avec les autocorrélogrammes ACF et PACF\n",
        "def tsplot(y, lags=None, figsize=(20, 10), style='bmh'):\n",
        "    \n",
        "  #Determing rolling statistics\n",
        "  window = 12\n",
        "  rolmean = y.rolling(window=window).mean()\n",
        "  rolstd = y.rolling(window=window).std()\n",
        "\n",
        "  #Plot rolling statistics:\n",
        "  fig = plt.figure(figsize=(15, 4))\n",
        "  orig = plt.plot(y.iloc[window:], color='blue',label='Original')\n",
        "  mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
        "  std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
        "  plt.legend(loc='best')\n",
        "  plt.title('Rolling Mean & Standard Deviation')\n",
        "  plt.show()\n",
        "\n",
        "  \"\"\"\n",
        "  Test de Dickey–Fuller\n",
        "  avec Autocorrélogrammes ACF et PACF\n",
        "\n",
        "  \"\"\"\n",
        "  # perform augmented Dickey-Fuller test\n",
        "  adf_test = adfuller(y, autolag='AIC')\n",
        "  print(f\"Results of Dickey-Fuller test :\\n{'-'*50}\")\n",
        "\n",
        "  dftest = adfuller(y, autolag='AIC')\n",
        "  dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','P-value','# Lags Used','Number of Observations Used'])\n",
        "  print(dfoutput)\n",
        "  print('\\n')\n",
        "  print(\"Is this data stationary ?\")\n",
        "  for key, value in dftest[4].items():\n",
        "    print(\"\\tCritical value {}: {} - The data is {} stationary with {}% confidence\".format(key, value, \"not\" if value < dftest[0] else \"\", 100-int(key[:-1])))\n",
        "    # dfoutput['Critical Value (%s)'%key] = value\n",
        "  # print (dfoutput)\n",
        "  # print(\"\\n\")\n",
        "\n",
        "  alpha = 0.05\n",
        "  p_value = adf_test[1]\n",
        "  ADF_h0 = \"CONCLUSION : time series is non-stationary, accept H0\"\n",
        "  ADF_ha = \"CONCLUSION : time series is stationary, reject H0\"\n",
        "\n",
        "  if p_value < alpha:\n",
        "    print(\"\\n\")\n",
        "    print(ADF_ha)\n",
        "  elif p_value > alpha:\n",
        "    print(\"\\n\")\n",
        "    print(ADF_h0)\n",
        "\n",
        "  # visualisation des autocorrélogrammes ACF et PACF\n",
        "  if not isinstance(y, pd.Series):\n",
        "      y = pd.Series(y)\n",
        "\n",
        "  with plt.style.context(style):\n",
        "      fig = plt.figure(figsize=figsize)\n",
        "      layout = (2, 2)\n",
        "      ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
        "      acf_ax = plt.subplot2grid(layout, (1, 0))\n",
        "      pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
        "\n",
        "      y.plot(ax=ts_ax)\n",
        "      p_value = sm.tsa.stattools.adfuller(y)[1]\n",
        "      ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
        "      sm.tsa.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
        "      sm.tsa.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
        "      plt.tight_layout()"
      ],
      "metadata": {
        "id": "NNi_BmCbAoU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decomposition\n",
        "# additive\n",
        "def add_decomp(variable):\n",
        "  cols = [\"{}\".format(col_name) for col_name in variable.columns]\n",
        "  str = f\"Additive decomposition for {cols}\"\n",
        "\n",
        "  add_decomp = seasonal_decompose(variable, model=\"additive\")\n",
        "  add_decomp.plot().suptitle(str, y=1.01, fontsize=15, fontweight=\"bold\")\n",
        "\n",
        "  add_decomp_results = pd.concat([add_decomp.seasonal, add_decomp.trend, add_decomp.resid, add_decomp.observed], axis=1)\n",
        "  add_decomp_results.columns = [\"seasonality\", \"trend\", \"residual\", \"actual_values\"]\n",
        "  print(\"Additive : \")\n",
        "  display(add_decomp_results.head())\n",
        "\n",
        "# multiplicative\n",
        "def mul_decomp(variable):\n",
        "  cols = [\"{}\".format(col_name) for col_name in variable.columns]\n",
        "  str = f\"Multiplicative decomposition for {cols}\"\n",
        "\n",
        "  mul_decomp = seasonal_decompose(variable, model=\"multiplicative\")\n",
        "  mul_decomp.plot().suptitle(str, y=1.01, fontsize=15, fontweight=\"bold\")\n",
        "\n",
        "  mul_decomp_results = pd.concat([mul_decomp.seasonal, mul_decomp.trend, mul_decomp.resid, mul_decomp.observed], axis=1)\n",
        "  mul_decomp_results.columns = [\"seasonality\", \"trend\", \"residual\", \"actual_values\"]\n",
        "  print(f\"{'-'*55}\\nMultiplicative :\")\n",
        "  display(mul_decomp_results.head())\n",
        "\n",
        "# stationarity test\n",
        "def sty_test(variable):\n",
        "\n",
        "  # # determining rolling stats\n",
        "  # ma_mean = variable.rolling(window=12, center=True).mean()\n",
        "  # ma_std = variable.rolling(window=12, center=True).std()\n",
        "\n",
        "  # # plot rolling stats\n",
        "  # orig = variable.plot(label=\"original data\")\n",
        "  # mean = ma_mean.plot(label=\"rolling mean\")\n",
        "  # # std = ma_std.plot(label=\"rolling std\")\n",
        "  \n",
        "  # plt.title(\"Smoothed time series on moving average\")\n",
        "  # plt.suptitle(\"window = 12\", y=0.9, fontsize=15, fontweight=\"bold\")\n",
        "  # plt.tight_layout()\n",
        "  # plt.legend()\n",
        "  # plt.show()\n",
        "\n",
        "  # perform augmented Dickey-Fuller test\n",
        "  adf_test = adfuller(y, autolag='AIC')\n",
        "  print(f\"Results of Dickey-Fuller test :\\n{'-'*50}\")\n",
        "\n",
        "  dftest = adfuller(y, autolag='AIC')\n",
        "  dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','P-value','# Lags Used','Number of Observations Used'])\n",
        "  print(dfoutput)\n",
        "  print('\\n')\n",
        "  print(\"Is this data stationary ?\")\n",
        "  for key, value in dftest[4].items():\n",
        "    print(\"\\tCritical value {}: {} - The data is {} stationary with {}% confidence\".format(key, value, \"not\" if value < dftest[0] else \"\", 100-int(key[:-1])))\n",
        "    # dfoutput['Critical Value (%s)'%key] = value\n",
        "  # print (dfoutput)\n",
        "  # print(\"\\n\")\n",
        "\n",
        "  alpha = 0.05\n",
        "  p_value = adf_test[1]\n",
        "  ADF_h0 = \"CONCLUSION : time series is non-stationary, accept H0\"\n",
        "  ADF_ha = \"CONCLUSION : time series is stationary, reject H0\"\n",
        "\n",
        "  if p_value < alpha:\n",
        "    print(\"\\n\")\n",
        "    print(ADF_ha)\n",
        "  elif p_value > alpha:\n",
        "    print(\"\\n\")\n",
        "    print(ADF_h0)\n",
        "\n",
        "# normality test\n",
        "def nrm_test(variable):\n",
        "  \n",
        "  # Shapiro-Wilk test\n",
        "  print(\"Results of Shapiro-Wilk test :\")\n",
        "  print(\"-\"*30)\n",
        "  sw_results = st.shapiro(variable)\n",
        "  sw_output = pd.Series(sw_results[0:2],\n",
        "                        index=[\"Test statistic\",\n",
        "                               \"p_value\"])\n",
        "  print(sw_output, \"\\n\")\n",
        "  \n",
        "  alpha = 0.05\n",
        "  p_value = sw_output[1]\n",
        "  SW_h0 = \"CONCLUSION : Distirbution is normal, accept H0\"\n",
        "  SW_ha = \"CONCLUSION : Distribution is non-normal, reject H0\"\n",
        "  \n",
        "  if p_value > alpha:\n",
        "    print(f\"p_value {p_value:.3f} > alpha\")\n",
        "  elif p_value < alpha:\n",
        "    print(f\"p_value {p_value:.3f} < alpha\")\n",
        "\n",
        "  if p_value > alpha:\n",
        "    # print(\"\\n\")\n",
        "    print(SW_h0)\n",
        "  elif p_value < alpha:\n",
        "    # print(\"\\n\")\n",
        "    print(SW_ha)\n",
        "  \n",
        "  print(\"\\n\")\n",
        "\n",
        "  # Kolmogorov-Smirnov test\n",
        "  print(\"Results of Kolmogorov-Smirnov test :\")\n",
        "  print(\"-\"*36)\n",
        "  ks_results = sm.stats.stattools.jarque_bera(variable)\n",
        "  ks_output = pd.Series(ks_results[0:2],\n",
        "                        index=[\"KS_stats\",\n",
        "                               \"p_value\"])\n",
        "  print(ks_output, \"\\n\")\n",
        "  \n",
        "  alpha = 0.05\n",
        "  p_value = ks_output[1]\n",
        "  KS_h0 = \"CONCLUSION : Data is normally distributed, accept H0\"\n",
        "  KS_ha = \"CONCLUSION : Data series is non-normal, reject H0\"\n",
        "  \n",
        "  if p_value > alpha:\n",
        "    print(f\"p_value {p_value:.3f} > alpha\")\n",
        "  elif p_value < alpha:\n",
        "    print(f\"p_value {p_value:.3f} < alpha\")\n",
        "\n",
        "  if p_value > alpha:\n",
        "    # print(\"\\n\")\n",
        "    print(KS_h0)\n",
        "  elif p_value < alpha:\n",
        "    # print(\"\\n\")\n",
        "    print(KS_ha)\n",
        "  \n",
        "  print(\"\\n\")\n",
        "\n",
        "  # Jarque-Bera test\n",
        "  print(\"Results of Jarque-Bera test :\")\n",
        "  print(\"-\"*30)\n",
        "  jb_results = sm.stats.stattools.jarque_bera(variable)\n",
        "  jb_output = pd.Series(jb_results[0:4],\n",
        "                        index=[\"JB_stats\",\n",
        "                               \"p_value\",\n",
        "                               \"Skewness\",\n",
        "                               \"Kurtosis\"])\n",
        "  print(jb_output, \"\\n\")\n",
        "  \n",
        "  alpha = 0.05\n",
        "  p_value = jb_output[1]\n",
        "  JB_h0 = \"CONCLUSION : Normal distribution, accept H0\"\n",
        "  JB_ha = \"CONCLUSION : Non-normal distribution, reject H0\"\n",
        "  \n",
        "  if p_value > alpha:\n",
        "    print(f\"p_value {p_value:.3f} > alpha\")\n",
        "  elif p_value < alpha:\n",
        "    print(f\"p_value {p_value:.3f} < alpha\")\n",
        "\n",
        "  if p_value > alpha:\n",
        "    # print(\"\\n\")\n",
        "    print(JB_h0)\n",
        "  elif p_value < alpha:\n",
        "    # print(\"\\n\")\n",
        "    print(JB_ha)\n",
        "  \n",
        "  sns.displot(variable, kde=True, bins=10, aspect=1.25, height=7)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "# residuals normality test\n",
        "def bp_test(variable):\n",
        "  print(\"Results of Breusch-Pagan test:\")\n",
        "  print(\"-\"*30)\n",
        "  bp_results = sm.stats.diagnostic.het_breuschpagan(variable.resid, variable.model.exog)\n",
        "  bp_output = pd.Series(bp_results[0:4],\n",
        "                        index=[\"Lagrange multiplier statistic\",\n",
        "                               \"p_value\",\n",
        "                               \"F value\",\n",
        "                               \"F p_value\"])\n",
        "  print(bp_output, \"\\n\")\n",
        "  \n",
        "  alpha = 0.05\n",
        "  p_value = bp_output[1]\n",
        "  BP_h0 = \"CONCLUSION : Homoscedasticity is present (the residuals are distributed with equal variance), accept H0\"\n",
        "  BP_ha = \"CONCLUSION : Heteroscedasticity is present (the residuals are not distributed with equal variance), reject H0\"\n",
        "  \n",
        "  if p_value > alpha:\n",
        "    print(f\"p_value {p_value:.3f} > alpha\")\n",
        "  elif p_value < alpha:\n",
        "    print(f\"p_value {p_value:.3f} < alpha\")\n",
        "\n",
        "  if p_value > alpha:\n",
        "    # print(\"\\n\")\n",
        "    print(BP_h0)\n",
        "  elif p_value < alpha:\n",
        "    # print(\"\\n\")\n",
        "    print(BP_ha)\n",
        "\n",
        "# Significance test\n",
        "def significance(x,y):\n",
        "\n",
        "  print(\"Results of Significance test :\")\n",
        "\n",
        "  slope, intercept, r_value, p_value, std_err = st.linregress(x,y)\n",
        "    \n",
        "  alpha = 0.05\n",
        "  p_value = p_value\n",
        "  sg_h0 = \"CONCLUSION : Non significant (X has no effect on Y), accept H0\"\n",
        "  sg_ha = \"CONCLUSION : Statistically significant (X has enough effect on Y), reject H0\"\n",
        "  \n",
        "  if p_value > alpha:\n",
        "    print(f\"p_value {p_value:.3f} > alpha\")\n",
        "  elif p_value < alpha:\n",
        "    print(f\"p_value {p_value:.3f} < alpha\")\n",
        "\n",
        "  if p_value > alpha:\n",
        "    print(\"-\"*30)\n",
        "    # print(\"\\n\")\n",
        "    print(sg_h0)\n",
        "  elif p_value < alpha:\n",
        "    print(\"-\"*30)\n",
        "    # print(\"\\n\")\n",
        "    print(sg_ha)"
      ],
      "metadata": {
        "id": "4nWY0pCjq14P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasets"
      ],
      "metadata": {
        "id": "cCT7b3EjK2-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/OC/P9/conso.csv\", index_col=\"Mois\", parse_dates=True)\n",
        "paris = pd.read_excel(\"/content/drive/MyDrive/OC/P9/DJU.xlsx\", skiprows=11)\n",
        "arima = cv2.imread('/content/drive/MyDrive/OC/P9/arima chart.jpeg')\n",
        "arima = cv2.resize(arima, (700, 350))"
      ],
      "metadata": {
        "id": "tSFbxYnCK6OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "LoQ3GGYsLZ6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### consommation"
      ],
      "metadata": {
        "id": "0_WSjo7cNndM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.index.names = [\"date\"]\n",
        "data = data[data[\"Territoire\"]==\"France\"][[\"Consommation totale\"]]\n",
        "data.rename(columns={\"Consommation totale\":\"raw\"}, inplace=True)\n",
        "\n",
        "# display(data.dtypes,\n",
        "#         data.head(2),\n",
        "#         data.shape,\n",
        "#         data.groupby([data.index.year])[[\"raw\"]].count(),\n",
        "#         data.index)"
      ],
      "metadata": {
        "id": "oPOS_6Y-Lb1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "QLIdfaF-S1g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.plot()"
      ],
      "metadata": {
        "id": "oUpf6CE9M0yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,16))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "sns.boxplot(data.index.year, data.raw)\n",
        "plt.title(\"Consommation d'éléctricité\\nannées 2012 - 2020\")\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "sns.boxplot(data.index.month, data.raw)\n",
        "plt.title(\"Consommation d'éléctricité\\nmenseulle entre 2012 et 2020\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jDcFcwP0gqNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.raw.resample(\"Y\").agg([\"mean\", \"median\", \"min\", \"max\", \"std\"])"
      ],
      "metadata": {
        "id": "RYduDt2QvEhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### degrés jours unifiés"
      ],
      "metadata": {
        "id": "qrmcN4fiNo5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paris.drop(columns=\"Total\", inplace=True)\n",
        "paris.rename(columns={\"Unnamed: 0\":\"date\",\n",
        "                          \"JAN\":\"01\",\n",
        "                          \"FÉV\":\"02\",\n",
        "                          \"MAR\":\"03\",\n",
        "                          \"AVR\":\"04\",\n",
        "                          \"MAI\":\"05\",\n",
        "                          \"JUN\":\"06\",\n",
        "                          \"JUI\":\"07\",\n",
        "                          \"AOÛ\":\"08\",\n",
        "                          \"SEP\":\"09\",\n",
        "                          \"OCT\":\"10\",\n",
        "                          \"NOV\":\"11\",\n",
        "                          \"DÉC\":\"12\"}, inplace=True)\n",
        "paris.drop(0, inplace=True)\n",
        "paris.set_index(\"date\", inplace=True)\n",
        "\n",
        "impute = {'month':[],'dju':[]}\n",
        "\n",
        "for year in paris.index.values:\n",
        "    for month in paris.columns:\n",
        "        impute['month'].append(f\"{year}-{month}\")\n",
        "        impute['dju'].append(paris.loc[year,month])\n",
        "        \n",
        "dju = pd.DataFrame(impute)\n",
        "dju['date'] = pd.to_datetime(dju['month'])\n",
        "dju.set_index(\"date\", inplace=True)\n",
        "dju.drop(columns=\"month\", inplace=True)\n",
        "\n",
        "# display(dju.dtypes,\n",
        "#         dju.head(2),\n",
        "#         dju.shape,\n",
        "#         dju.groupby([dju.index.year])[[\"dju\"]].count(),\n",
        "#         dju.index)"
      ],
      "metadata": {
        "id": "CxVlm-eMMgSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dju.head()"
      ],
      "metadata": {
        "id": "mznv1K1sVXeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dju.plot()"
      ],
      "metadata": {
        "id": "m_80g5fcOjnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dju.dju.resample(\"Y\").agg([\"mean\", \"median\", \"min\", \"max\", \"std\"])"
      ],
      "metadata": {
        "id": "GB51HW5svCFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### merge"
      ],
      "metadata": {
        "id": "ZBWHoM4WNtZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.merge(data, dju, on=\"date\", how=\"inner\")\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "H4b0TYgXNcZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data overview"
      ],
      "metadata": {
        "id": "jMnpAG3CayPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "YJNwapWRfJTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(2,1,1)\n",
        "df.resample(\"Y\")[\"raw\"].plot(title=\"Consommation d'électricité par an\")\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "df.resample(\"Y\")[\"dju\"].plot(title=\"Degré Jour Unifié\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WdeGwecJa9FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data correction"
      ],
      "metadata": {
        "id": "WrL-crXQhCYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### finding the coefficient term from Linear Regression using OLS Summary"
      ],
      "metadata": {
        "id": "XHAO_JcMh5ZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va pouvoir effectuer une corrélation entre les variables raw et DJU. Ainsi, on obtiendra un modèle de régression linéaire qui nous permettra de corriger la consommation d'électricité en fonction de la température enregistrée."
      ],
      "metadata": {
        "id": "LqIEes4MYltu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# linear regression plot\n",
        "x = np.array(df['dju'])\n",
        "y = np.array(df['raw'])\n",
        "\n",
        "sns.regplot(x=x,\n",
        "            y=y,\n",
        "            line_kws={\"color\":\"k\", \"lw\":2},\n",
        "            scatter_kws={\"color\":\"darkorange\", \"alpha\":0.7, \"s\":20})\n",
        "plt.title(\"Regression lineaire :\\nconsommation d'électricité et DJU\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hdnXfZ8-rPr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df[[\"raw\", \"dju\"]].corr())\n",
        "print(\"\\n\")\n",
        "\n",
        "reg = smf.ols('raw ~ dju', data=df).fit()\n",
        "print(reg.summary(), \"\\n\")\n",
        "\n",
        "significance(x,y)"
      ],
      "metadata": {
        "id": "WFT9q1xXhGM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notre régression linéaire montre un coefficient de détermination de 0.946%. On en conclut qu'il y a une forte corrélation entre la consommation totale et la température en DJU. En effet, DJU explique ~95% de la consommation.\n",
        "\n",
        "De plus, l'observation de notre graphique montre que cette corrélation est positive (plus la consommation augmente plus la différence de température est élevé).\n",
        "\n",
        "Aussi, la p-value (0.00) est inférieure à 0.05 donc notre corrélation est statistiquement significative.\n",
        "\n",
        "- **R-squared value:** $R^2$ is the coefficient of determination that tells us that how much percentage variation independent variable can be explained by independent variable. Here, 94.6 % variation in consommation can be explained by DJU. The maximum possible value of $R^2$  can be 1, means the larger the $R^2$  value better the regression.\n",
        "\n",
        "- **Coefficient term:** The coefficient term tells the change in \"consommation\" for a unit change in \"DJU\". Therefore, if DJU rises by 1 unit, then consommation rises by 49.0051. If you are familiar with derivatives then you can relate it as the rate of change of DJU with respect to consommation."
      ],
      "metadata": {
        "id": "tX6l4sIPio8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### residuals normality test"
      ],
      "metadata": {
        "id": "_Y92If86p2Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QQ plot\n",
        "st.probplot(reg.resid, plot= plt)\n",
        "plt.title(\"QQ plot\"); plt.show()\n",
        "\n",
        "# homoscedasticity test\n",
        "print(\"HOMOSCEDASTICITY TEST\")\n",
        "display(bp_test(reg))\n",
        "print(\"\\n\")\n",
        "\n",
        "# gaussian distribution test\n",
        "print(\"GAUSSIAN DISTRIBUTION TEST\")\n",
        "display(nrm_test(reg.resid))"
      ],
      "metadata": {
        "id": "SMsnp0E8p55W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut identifier une distribution des résidus satisfaisante, alignée avec la distribution théorique d'une loi normale. Un test de normalité est nécessaire pour valider ou rejeter cette intuition. Or, la p_value supérieure au seuil ne permet pas de rejeter l'hypothèse nulle H0 de normalité des résidus, le test est donc validé: on accept H0 (la distribution est gaussienne)."
      ],
      "metadata": {
        "id": "x7nIVkBucDuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### corrected consumption"
      ],
      "metadata": {
        "id": "DE0C1A-jrapI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée une nouvelle colonne dans notre dataframe qui contient les valeurs de la consommation totale corrigée grâce au paramètre de la régression linéaire que l'on a extrait plus tôt."
      ],
      "metadata": {
        "id": "eT-pzFh0rkDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# correction des données avec la regression lineaire\n",
        "coef = reg.params[\"dju\"]\n",
        "\n",
        "df[\"conso\"] = df.raw - df.dju * coef\n",
        "display(df.dtypes,\n",
        "        df.head(3),\n",
        "        df.shape)"
      ],
      "metadata": {
        "id": "Vkp8Vh5NrS4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal data"
      ],
      "metadata": {
        "id": "Zj12Wy9lsxDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data overview"
      ],
      "metadata": {
        "id": "f1zJS-qCtHWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "17TRV4oMr0ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois la correction de la consommation calculée, on la représente graphiquement pour voir la différence entre consommation totale et consommation totale corrigée de la température."
      ],
      "metadata": {
        "id": "nBHcamE1aB2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(3,1,1)\n",
        "df.resample(\"Y\")[\"conso\"].plot(title=\"Consommation (corrigée)\")\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "df.resample(\"Y\")[\"raw\"].plot(title=\"Consommation (non corrigée)\")\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "df.resample(\"Y\")[\"dju\"].plot(title=\"Degré Jour Unifié\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_6l6hyZtU3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"raw\", \"conso\"]].plot()\n",
        "plt.title(\"avant et après correction\\nde la consommation d'electricité\"); plt.show()\n",
        "\n",
        "df[[\"raw\", \"conso\"]].plot(subplots=True); plt.show()"
      ],
      "metadata": {
        "id": "zL72W5XEWiJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset trend shows that it is a kind of additive time series, but this feature dataset has also a seasonality in it."
      ],
      "metadata": {
        "id": "o0FkHR49tW45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(27,16))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "sns.boxplot(df.index.year, df.conso)\n",
        "plt.title(\"Consommation corrigée\\nannées 2012 - 2020\")\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "sns.boxplot(df.index.year, df.raw)\n",
        "plt.title(\"Consommation non-corrigée\\nannées 2012 - 2020\")\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "sns.boxplot(df.index.month, df.conso)\n",
        "plt.title(\"Consommation corrigée (mensuelle)\\nannées 2012 - 2020\")\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "sns.boxplot(df.index.month, df.raw)\n",
        "plt.title(\"Consommation non-corrigée (mensuelle)\\nannées 2012 - 2020\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h3fFvSVptW1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En ayant corrigé notre série temporelle sur la consommation d'électricité, on s'assure que les prédictions faites dans la suite de notre étude ne prenne en compte que les variations de consommation électrique sans être influencés par les variations de température extérieures."
      ],
      "metadata": {
        "id": "DNE1dvx3XekW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guassian distribution test"
      ],
      "metadata": {
        "id": "9q9E3MGdXhk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nrm_test(df.conso)"
      ],
      "metadata": {
        "id": "MOxpyqS0XzPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decomposition"
      ],
      "metadata": {
        "id": "cqDqeUrHbLhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois la correction effectuée, on peut s'intéresser aux différents composants de la série temporelle."
      ],
      "metadata": {
        "id": "0dD47akTdN4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decompose_data = seasonal_decompose(df.conso, model=\"additive\")\n",
        "decompose_data.plot();"
      ],
      "metadata": {
        "id": "wQFU5fEgcIW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here in the above chart, we can see the decomposed structure of data and the structure of the components in the data set which were affecting it. Let’s make a graph for available seasonality."
      ],
      "metadata": {
        "id": "AwQ718ofeGvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seasonality"
      ],
      "metadata": {
        "id": "Kpvq6785d0Xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s check for a couple of years seasonality to know the similarity more accurately. I am choosing the years 2012, 2015, 2017, and 2020 for the test."
      ],
      "metadata": {
        "id": "LIhRunoPd11i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(4)\n",
        "\n",
        "ax[0].plot(df.loc[\"2012\", \"conso\"], label='2012')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(df.loc[\"2015\", \"conso\"], label='2015')\n",
        "ax[1].legend()\n",
        "\n",
        "ax[2].plot(df.loc[\"2017\", \"conso\"], label='2017')\n",
        "ax[2].legend()\n",
        "\n",
        "ax[3].plot(df.loc[\"2020\", \"conso\"], label='2020')\n",
        "ax[3].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0mHMpSwXd5gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seasonality = decompose_data.seasonal\n",
        "seasonality.plot(c=\"red\", title=\"seasonality\"); plt.legend(); plt.show()"
      ],
      "metadata": {
        "id": "sL_iWfyneLVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the seasonality graph, we can see the seasonality structure for every year, which is cyclic and repeatedly providing the same value."
      ],
      "metadata": {
        "id": "JUzqUZ6CeP-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deseasonalisation"
      ],
      "metadata": {
        "id": "3jgTrDAqcDbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# différence avec la saisonnalité pour obtenir une série temporelle hors impact saisonnier\n",
        "deseasonalized = df.conso - seasonality\n",
        "moving_average = df.conso.rolling(window=12, center=True).mean()\n",
        "\n",
        "# visualisation de la consommation en électricité avant et après désaisonnalisation\n",
        "plt.plot(df['conso'], label='Consommation corrigée')\n",
        "plt.plot(deseasonalized, label='Consommation désaisonnalisée')\n",
        "plt.plot(moving_average, label='moyenne mobile')\n",
        "\n",
        "plt.suptitle('window = 12', fontsize=15, y=0.86)\n",
        "plt.title('Avant et après désaisonnalisation')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vEjqsaHhe--N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Differencing"
      ],
      "metadata": {
        "id": "EwgeS4LiD5us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsplot(df.conso)"
      ],
      "metadata": {
        "id": "mJt-UkD2DG_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diff = df.conso - df.conso.shift(1)\n",
        "tsplot(diff.dropna())"
      ],
      "metadata": {
        "id": "h1UPJZJSDTpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seasonal_diff = df.conso - df.conso.shift(12)\n",
        "seasonal_diff = df.conso - df.conso.shift(1)\n",
        "tsplot(seasonal_diff.dropna())"
      ],
      "metadata": {
        "id": "IetDYXsVDqew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prévision de la consommation sur un an"
      ],
      "metadata": {
        "id": "9dfwEDrPnXeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La phase de prédition se fera en utilisant la méthode de Holt-Winters (lissage exponentiel) puis la méthode SARIMA."
      ],
      "metadata": {
        "id": "nF3Y0W0iuDwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Holt-Winters method"
      ],
      "metadata": {
        "id": "UVasKsMAoo0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La méthode de Holt-Winters génère des valeurs lissées de façon exponentielle pour le niveau, la tendance et l'ajustement saisonnier de la série temporelle. Cette méthode convient pour les séries temporelles dont la saisonnalité et la tendance restent stables dans le temps.\n",
        "\n",
        "La méthode ExponentialSmoothing de statsmodels est utilisée pour modéliser le lissage exponentiel d'Holt-Winters.\n",
        "\n",
        "On commence par définir les paramètres de notre modèle puis on le représente graphiquement. Ici les paramètres de notre modèle contiennent des informations sur la saisonnalité qui est égale à 12 et une tendance et une saisonnalité additives."
      ],
      "metadata": {
        "id": "GdObLOUSuCAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "holt_winters = ExponentialSmoothing(np.asarray(df[\"conso\"]), seasonal_periods=12, trend='add', seasonal='add').fit()\n",
        "predict_hw = holt_winters.forecast(12)\n",
        "\n",
        "plt.plot(df[\"conso\"], label='Consommation corrigée')\n",
        "plt.plot(pd.date_range(df.index[len(df)-1], periods=12, freq='M'), predict_hw, label='Prévision Holt-Winters')\n",
        "\n",
        "plt.title(\"2021 Holt-Winters Forecasting\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EHdNiIGaiPht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La méthode de Holt-Winters est la plus raisonnable des méthodes de lissage exponentiel. La prévision sur un an de la consommation, corrigée de l'effet température, tient compte de la saisonnalité."
      ],
      "metadata": {
        "id": "mIQZQbInt-s8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Holt-Winters :  analyse a posteriori"
      ],
      "metadata": {
        "id": "hpFO0ntKudF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois les paramètres de notre modèle définis, on évalue notre modèle. Pour cela, on enlève les données des 12 derniers mois dont on dispose puis on compare les valeurs enregistrées avec les valeurs prédites de nottre modèle.\n",
        "\n",
        "On tronque la série de l’année 2020, qu’on cherche ensuite à prévoir à partir de l’historique 2012-2019. Cette analyse permet d'avoir une idée de la qualité prédictive du modèle choisi."
      ],
      "metadata": {
        "id": "M2DsmZ8QutAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set(df.index.year)"
      ],
      "metadata": {
        "id": "5Rk1bVhZvFJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the train/test split length\n",
        "split = int(len(df)*.89)\n",
        "train = df[:split]\n",
        "test = df[split:]\n",
        "\n",
        "# visualization of train/test splitting\n",
        "train.conso.plot()\n",
        "test.conso.plot(figsize=(20, 6), title= 'Train - Test splitting', fontsize=14, c='r');"
      ],
      "metadata": {
        "id": "KYNmN6GBtthR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_hw = ExponentialSmoothing(np.asarray(train[\"conso\"]), seasonal_periods=12, trend='add', seasonal='add').fit()\n",
        "train_pred = train_hw.forecast(12)\n",
        "\n",
        "plt.plot(df[\"conso\"], label=\"Consommation corrigée\")\n",
        "plt.plot(pd.date_range(train.index[len(train)-1], periods=12, freq='M'), train_pred, label=\"Prévision Holt-Winters\")\n",
        "\n",
        "plt.title(\"Holt-Winters Model Evaluation\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OJRIMnjRvknF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Holt-Winters : model evaluation"
      ],
      "metadata": {
        "id": "we_nKSOJy1xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graphiquement, on remarque que la prédiction a réussi à capturer la saisonnalité mais qu'il a du mal à prédire la baisse importante du début de l'année 2020. Comme vu dans la définition de la méthode de Holt-Winters, ce modèle est bon pour prédire des événements lorsque la saisonnalité et la tendance sont stables. Or, selon la décomposition de notre série temporelle on remarque que la tendance est stable jusque 2019 mais qu'ensuite, elle s'effondre au début de l'année 2020.\n",
        "\n",
        "Pour pouvoir conclure à l'évaluation de notre modèle, on va ensuite calculer différents indices."
      ],
      "metadata": {
        "id": "MRS-dEDly7x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = train_pred\n",
        "actual_values = np.asarray(train['conso'].iloc[-12:])\n",
        "\n",
        "mae = mean_absolute_error(actual_values, train_pred)\n",
        "mse = mean_squared_error(actual_values, train_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mape = np.mean(np.abs((actual_values - train_pred) / actual_values)) * 100\n",
        "\n",
        "print('MAE = ', mae)\n",
        "print('MSE = ', mse)\n",
        "print('RMSE = ', rmse)\n",
        "print('MAPE = ', mape)"
      ],
      "metadata": {
        "id": "Ihnpq-g1y6ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notre modèle obtient des scores plutôt bons avec une marge d'erreur (MAPE) de 1.75%. Cependant ce type de modélisation n'est pas adapté pour notre série temporelle où la tendance est à la baisse au début de l'année 2020."
      ],
      "metadata": {
        "id": "DxR0iH0I0O1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting up SARIMA"
      ],
      "metadata": {
        "id": "uwKY3DOz0xcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour contrer le problème de la difficulté de traitement de la présence d'une tendance et/ou d'une saisonnalité changeante, on va pouvoir utiliser un modèle issu de la méthode SARIMA. Cette méthode combine une méthode auto-regressive à une méthode de moyenne mobile.\n",
        "\n",
        "Plusieurs étapes sont nécessaires :\n",
        "\n",
        "- Identifier/confirmer la saisonnalités (autocorrélogrammes)\n",
        "- Stationnariser la série temporelle (différenciation)\n",
        "- Déterminer des ordres optimaux plausibles\n",
        "- Estimer les paramètres et les départager par l’AIC (ou le BIC)\n",
        "- Valider ou non le modèle par un diagnostique des résidus (test, représentation graphique, autocorrélogramme)\n",
        "- Confirmer le(s) choix en simulant la prévision.\n",
        "\n"
      ],
      "metadata": {
        "id": "9urSxfgS01C_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Stationarity"
      ],
      "metadata": {
        "id": "jmf3xZZA1eR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une série présentant une tendance et/ou une saisonnalité ne pourra pas être modélisée par un processus stationnaire. Une technique communément employée est de travailler non pas sur la série mais sur des différences de la série."
      ],
      "metadata": {
        "id": "WPe8dgavJYY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va devoir vérifier la non-stationnarité de notre série temporelle avant de pouvoir réaliser la prévision par SARIMA. On commence donc par créer une variable qui ne contient que notre série temporelle de la consommation corrigée."
      ],
      "metadata": {
        "id": "BTu6LCFQ1ruu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df[\"conso\"]"
      ],
      "metadata": {
        "id": "WjmMoUXX1rV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va ensuite s'intéresser à la saisonnalité et au calcul de la stationnarité de notre série temporelle."
      ],
      "metadata": {
        "id": "ps-H4SKu1_2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsplot(y, lags=55)"
      ],
      "metadata": {
        "id": "4yrXlQcv2L_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La sortie ACF présente une décroissance lente vers 0, ce qui traduit un problème de non-stationnarité. Confirmation également par le test de Dickey-Fuller portant pour hypothèse nulle la non-stationnarité de la série, ne pouvant pas être rejetée. On effectue donc une première différenciation."
      ],
      "metadata": {
        "id": "NiWWR21SkQzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff = df.conso - df.conso.shift(1)\n",
        "tsplot(diff.dropna(), lags=60)"
      ],
      "metadata": {
        "id": "CGoiYUYFaetD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La sortie ACF de la série ainsi différenciée présente encore une décroissance lente vers 0 pour les multiples de 12. On effectue cette fois la différenciation d'ordre 12."
      ],
      "metadata": {
        "id": "U4ArXrLclXof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff_2 = diff - diff.shift(12)\n",
        "tsplot(diff_2.dropna(), lags=60)"
      ],
      "metadata": {
        "id": "V6YSM7bKk_5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette fois-ci les résultats sont satisfaisants, nous pouvons nous appuyer sur les autocorrélogrammes simple et partiels estimés."
      ],
      "metadata": {
        "id": "n7EDdvdflVHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Configuring parameters"
      ],
      "metadata": {
        "id": "_YYJOJCLtTK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = d = q = range(0, 2)\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
        "# print('Examples of parameter for SARIMA...')\n",
        "# print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
        "# print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
        "# print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
        "# print('SARIMAX: {} x {}\\n'.format(pdq[2], seasonal_pdq[4]))\n",
        "\n",
        "run_res = []\n",
        "\n",
        "for param in pdq:\n",
        "    for param_seasonal in seasonal_pdq:\n",
        "        try:\n",
        "            mod = sm.tsa.statespace.SARIMAX(y,\n",
        "                                            order=param,\n",
        "                                            seasonal_order=param_seasonal,\n",
        "                                            enforce_stationarity=False,\n",
        "                                            enforce_invertibility=False)\n",
        "            results = mod.fit()\n",
        "            run_res.append({'order':param,\n",
        "                            'seasonal': param_seasonal,\n",
        "                            'aic': results.aic,\n",
        "                            'mse': mse,\n",
        "                            'rmse': sqrt(mse)\n",
        "                           })\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "run_res_df = pd.DataFrame(run_res)\n",
        "run_res_df"
      ],
      "metadata": {
        "id": "eT-zCqw1MAY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Evaluating the model"
      ],
      "metadata": {
        "id": "XDiA60uGtheC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min = run_res_df.aic.min()\n",
        "run_res_df[run_res_df[\"aic\"] == min]"
      ],
      "metadata": {
        "id": "qPn16wYcXx9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max = run_res_df.aic.max()\n",
        "run_res_df[run_res_df[\"aic\"] == max]"
      ],
      "metadata": {
        "id": "SNFeN5wCXzSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_res_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "zQCtD7HgX1Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According Peterson, T. (2014) the AIC (Akaike information criterion) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. The low AIC value the better. Our output suggests that SARIMAX(0, 1, 1)\t(0, 1, 1, 12) with AIC value of 1367.02 is the best combination, so we should consider this to be optimal option.\n",
        "\n",
        "In the “mod = sm.tsa.statespace.SARIMAX” command we need to set up the chosen combination."
      ],
      "metadata": {
        "id": "eC4gszh4ZcYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod = sm.tsa.statespace.SARIMAX(y,\n",
        "                                order=(0, 1, 1),\n",
        "                                seasonal_order=(0, 1, 1, 12),\n",
        "                                enforce_stationarity=False,\n",
        "                                enforce_invertibility=False)\n",
        "results = mod.fit()\n",
        "# print(results.summary().tables[1])\n",
        "print(\"\\n\", results.summary())"
      ],
      "metadata": {
        "id": "LoOqGBZmZ9h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selon le test de significativité des paramètres obtenu dans le modèle, on constate que les coefficients/paramètres MA simple et MA saisonnière sont tous significativement différents de 0, à un niveau de test de 5%. En effet, leur p_value est inférieure à 5% niveau de test."
      ],
      "metadata": {
        "id": "ZiASlz2ocd3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### White noise"
      ],
      "metadata": {
        "id": "ttc2XgXxviTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "À l'issue de la modélisation, on va espérer sue le résidu obtenu soit un bruit blanc. C'est-à-dire, qu'on ait pu extraire au moins toutes les dépendances linéaires temporelles au sein de la série temporelle initale.  "
      ],
      "metadata": {
        "id": "EG-QPiDkviT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si nous avions voulu faire un test de blancheur du résidu de ce modèle, Nous pouvions utiliser la méthode de Ljung-Box."
      ],
      "metadata": {
        "id": "IWoZaaDdeLX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Retard : p-value')\n",
        "for elt in [6, 12, 18, 24, 30, 36]:\n",
        "    print('{} : {}'.format(elt, acorr_ljungbox(results.resid, lags=elt)[1].mean()))"
      ],
      "metadata": {
        "id": "0OWnaHe3d2_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On obtient ici les p_values pour les ordres 6 à 36, et pour rappel, le test effectué est un test de Ljung-Box. On constate là, que les p_values ne sont pas inférieures au nivea de test de 5%, donc on ne rejette pas l'hypothèse nulle (H0: le résidu suit un bruit blanc). Donc c'est plutôt salvateur.\n",
        "\n",
        "Ce modèle là est du coup satisfaisant pour les résidus, mais également pour les paramètres.\n",
        "\n",
        "> **RECAP :** les deux coefficients de nos paramètres $\\theta1$ et $\\theta1_{ saisonnier}$ sont très significatifs. Et pour que le modèle soit correcte, on a vérifié que le test de blancheur ne soit pas rejeté, et selon le test de Ljung-Box, effectivement le test de blancheur n'est pas rejeté, donc les résidus suivent un bruit blanche."
      ],
      "metadata": {
        "id": "A3wAuHbfqSt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Residual normality tests"
      ],
      "metadata": {
        "id": "BIUGcZDftqFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le cadre de prévisions, il convient de vérifier la normalité des résidus pour tester l'adéquation de nos modèles. La normalité peut-être détectée de façon graphique, mais des tests statistiques amènent un point de vue objectif non négligeable."
      ],
      "metadata": {
        "id": "C25f9YeAaZN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results.plot_diagnostics(figsize=(20,8))\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xHU1HZdaaY3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the diagnostic above we can visualize important information as the distribution and the Auto correlation function ACF (correlogram). Values upward the “0” has some correlation over the time series data. Values near to “1” demonstrates strongest correlation."
      ],
      "metadata": {
        "id": "JedUgFqustQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La représentation \"Standardized residual\" et \"Correlogram\" confirment qu'il n'y a pas de corrélation des résidus. Les résidus sont normalement distribués KDE vs distribution normale - N (0,1). La distribution ordonnée des résidus représentée par le du Q-Q plot est globalement satisfaisant, il y a quand même des petites divergences vers les queues de distribution. Il est intéressant de coupler l'approche visuelle par des tests statistiques."
      ],
      "metadata": {
        "id": "EG7ycPgbq9Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nrm_test(results.resid)"
      ],
      "metadata": {
        "id": "GpsjsExWqTcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce test ne prend pas en compte les premiers résidus d'après la documentation, ce qui rappelle le premier constat du Q-Q plot, les premiers résidus divergent légèrement d'un schéma gaussien, le test ne les prend pas en compte est devient donc concluant sans pouvoir rejeter l'hypothèse nulle de normalité des résidus dans ce cadre précis.\n",
        "\n",
        "Ici, l'hypothèse de normalité est remise en cause par Shapiro (p-value < 0.05).\n",
        "\n",
        "Néanmoins, l'observation des résidus, le fait qu'ils ne soient pas très différents d'une distribution symétrique, et le fait que l'échantillon soit de taille suffisante permettent de dire que les résultats obtenus par le modèle ne sont pas absurdes, même si le résidu n'est pas considéré comme étant gaussien."
      ],
      "metadata": {
        "id": "ZvLfutGMregG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SARIMA method"
      ],
      "metadata": {
        "id": "76y5j1totx2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les modèles SARIMA permettent de modéliser des séries qui présentent une saisonnalité.\n",
        "\n",
        "Estimer un modèle SARIMA se ramène en pratique à l'estimation d'un modèle ARMA sur la série différenciée."
      ],
      "metadata": {
        "id": "2hsMMV21UJbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = results.get_forecast(12)\n",
        "pred = model.predicted_mean\n",
        "pred_l = model.conf_int(alpha=0.03)\n",
        "pred_u = model.conf_int(alpha=0.03)\n",
        "\n",
        "plt.plot(y, label='consommation')\n",
        "plt.plot(pd.date_range(y.index[len(y)-1], periods=12, freq='M'), pred, color='red', label='prévision')\n",
        "plt.plot(pd.date_range(y.index[len(df)-1], periods=12, freq='M'), pred_l, color='gray', linestyle=':')\n",
        "plt.plot(pd.date_range(y.index[len(df)-1], periods=12, freq='M'), pred_u, color='gray', linestyle=':')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ieEUhEzHsdW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SARIMA :  analyse a posteriori"
      ],
      "metadata": {
        "id": "tlj_YHYUXOqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'analyse a posteriori permet de quantifier les écarts entre les prévisions et les réalisations, en tronquant la série d'un certain nombre de points (notons que le modèle doit être correctement estimé sur la série tronquée."
      ],
      "metadata": {
        "id": "0GnyM_LHW8Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pred = results.get_prediction(start=pd.to_datetime('2019-01-01'), dynamic=False)\n",
        "# pred_ci = pred.conf_int()\n",
        "# ax = y['2012':].plot(label='observed')\n",
        "# pred.predicted_mean.plot(ax=ax, label='one-step ahead Forecast', alpha=.7, figsize=(20, 8))\n",
        "# ax.fill_between(pred_ci.index,\n",
        "#                 pred_ci.iloc[:, 0],\n",
        "#                 pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
        "# ax.set_xlabel('Date')\n",
        "# ax.set_ylabel('consommation')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "UzNA_iFBvSQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_actual_values = y[-12:]\n",
        "past_values = y.shift(-12).dropna()\n",
        "\n",
        "model = SARIMAX(past_values, order=(0,1,1), seasonal_order=(0,1,1,12))\n",
        "model_results = model.fit()\n",
        "print(\"\\n\", model_results.summary())\n",
        "\n",
        "print('\\n', 'Retard : p-value')\n",
        "for elt in [6, 12, 18, 24, 30, 36]:\n",
        "    print('{} : {}'.format(elt, acorr_ljungbox(model_results.resid, lags=elt)[1].mean()))"
      ],
      "metadata": {
        "id": "qgUQMcN97hnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les tests de significativité des paramètres et de blancheur du résidu sont validés au niveau 5%."
      ],
      "metadata": {
        "id": "SCy_79gUllQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_trunc_model = model_results.get_forecast(12)\n",
        "pred_trunc = pred_trunc_model.predicted_mean\n",
        "pred_l_trunc = pred_trunc_model.conf_int(alpha=0.05)\n",
        "pred_u_trunc = pred_trunc_model.conf_int(alpha=0.05)\n",
        "\n",
        "plt.plot(past_values, label='consommation')\n",
        "plt.plot(pred_actual_values, label = 'consommation actuelle', color='red', lw=3, alpha=0.45)\n",
        "plt.plot(pd.date_range(past_values.index[len(past_values)-1], periods=12, freq='M'), pred_trunc, color='k', label='consmmation prédit', linestyle='--')\n",
        "plt.plot(pd.date_range(past_values.index[len(past_values)-1], periods=12, freq='M'), pred_l_trunc, color='g', linestyle=':', lw=0.75)\n",
        "plt.plot(pd.date_range(past_values.index[len(past_values)-1], periods=12, freq='M'), pred_u_trunc, color='g', linestyle=':', lw=0.75)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aux9H3B470hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zoom sur l'année 2019 à 2020"
      ],
      "metadata": {
        "id": "IBqS_04S4gk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_trunc_model = model_results.get_forecast(12)\n",
        "pred_trunc = pred_trunc_model.predicted_mean\n",
        "pred_l_trunc = pred_trunc_model.conf_int(alpha=0.05)\n",
        "pred_u_trunc = pred_trunc_model.conf_int(alpha=0.05)\n",
        "\n",
        "\n",
        "plt.plot(pred_actual_values, label = 'consommation actuelle', color='red', lw=3, alpha=0.45)\n",
        "plt.plot(pd.date_range(past_values.index[len(past_values)-1], periods=12, freq='M'), pred_trunc, color='k', label='consmmation prédit', linestyle='--')\n",
        "plt.plot(pd.date_range(past_values.index[len(past_values)-1], periods=12, freq='M'), pred_l_trunc, color='g', linestyle=':', lw=0.75)\n",
        "plt.plot(pd.date_range(past_values.index[len(past_values)-1], periods=12, freq='M'), pred_u_trunc, color='g', linestyle=':', lw=0.75)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W0AsBfiBCHyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SARIMA : model evaluation"
      ],
      "metadata": {
        "id": "BPQvtNSREB9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilise des critères d'erreur comme\n",
        "- l'erreur quadratique moyenne (RMSE = Root Mean Square Error)\n",
        "- ou\n",
        "- l'erreur relative absolue moyenne (MAPE = Mean Average Percentage Error)"
      ],
      "metadata": {
        "id": "5x0cA97-EJKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae = mean_absolute_error(pred_actual_values, pred_trunc)\n",
        "mse = mean_squared_error(pred_actual_values, pred_trunc)\n",
        "rmse = np.sqrt(mse)\n",
        "mape = np.mean(np.abs((pred_actual_values - pred_trunc) / pred_trunc)) * 100\n",
        "\n",
        "print('MAE = ', mae)\n",
        "print('MSE = ', mse)\n",
        "print('RMSE = ', rmse)\n",
        "print('MAPE = ', mape)"
      ],
      "metadata": {
        "id": "xR81klMCAJAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L’interprétation des critères d’erreur dépend de la série et de la qualité de prévision exigée. Dans le cas présent, un MAPE de 0.9% semble satisfaisant a priori.\n",
        "\n",
        "Prévision à 12 mois performante, la méthode SARIMA semble mieux prendre en compte les impacts saisonniers (pics et creux) et se montre plus performante que la méthode Holt-Winters où le MAPE était égale à 1.75%."
      ],
      "metadata": {
        "id": "GNJBCIYQDOoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ],
      "metadata": {
        "id": "VsRm0MOIDp9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette étude, nous avons utilisé les données mensuelles de consommation d’électricité en France, ainsi que l'effet de température lié au chauffage pour tester des modèles de prévison de cette consommation à 12 mois. L’exploration de ces données nous a révélé des caracteristiques tels que la non stationnarité de la série de consommation d’électricité et sa très forte inertie (autocorrélation forte et longue).\n",
        "\n",
        "Les prévisions par la méthode de lissage exponentiel Holt-Winters et SARIMA sont globablement satisfaisantes, avec l’erreur absolue moyenne en pourcentage au dessous de 2%. Par ailleurs, le modèle SARIMA sera retenu prioritairement pour prédire la consommation d’électricité plus ponctuelle (à court terme), il sera plus performant et plus robuste dans des prévisions devant prendre en compte de forts impacts saisonniers."
      ],
      "metadata": {
        "id": "Q_sP0tPJDrqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources"
      ],
      "metadata": {
        "id": "_ZlxJrsdCLou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Skewness and Kurtosis in finance](https://www.youtube.com/watch?v=WjdpbO3s-sQ&list=TLPQMzEwMzIwMjK1aBgSdlrfHw&index=2&ab_channel=DataCamp)\n",
        "2. [ARMA : understanding ACF and PACF charts](https://medium.com/@ooemma83/how-to-interpret-acf-and-pacf-plots-for-identifying-ar-ma-arma-or-arima-models-498717e815b6)\n",
        "3. [Matplotlib styling](https://matplotlib.org/2.1.2/api/_as_gen/matplotlib.pyplot.plot.html)\n",
        "04. [OLS summary](https://www.geeksforgeeks.org/interpreting-the-results-of-linear-regression-using-ols-summary/)\n",
        "05. [Simple linear regression excercice](https://www.youtube.com/watch?v=8jazNUpO3lQ&ab_channel=codebasics)\n",
        "6. [Normality tests](https://towardsdatascience.com/6-ways-to-test-for-a-normal-distribution-which-one-to-use-9dcf47d8fa93)\n",
        "7. [Homoscedasticity normality test for residuals](https://www.statology.org/breusch-pagan-test/)\n",
        "8. [Perform Breusch-Pagan test](https://www.statology.org/breusch-pagan-test-python/)\n",
        "9. [Residual plot](https://www.statology.org/residual-plot-python/)\n",
        "10. [Customizing Matplotlib with style sheets and rcParams](https://matplotlib.org/stable/tutorials/introductory/customizing.html)\n",
        "11. [Writing mathematical expressions in Matplotlib](https://matplotlib.org/3.5.0/tutorials/text/mathtext.html)"
      ],
      "metadata": {
        "id": "YymI8GAvCOI9"
      }
    }
  ]
}